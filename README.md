# ğŸš€ PySpark with Databricks â€“ Complete Learning Journey

## ğŸ“Œ Overview
This repository tracks my **step-by-step learning journey of PySpark using Databricks**.  
Learning is documented using **numbered PDF notes** (concepts) and **corresponding practice code files** (hands-on).

Each numbered set represents **one learning phase**, making it easy for visitors to follow the progression.

---

## ğŸ¯ Purpose of This Repository
- Maintain a **structured PySpark learning record**
- Combine **theory (PDF notes)** with **practice (code)**
- Apply concepts on **real-world datasets**
- Build strong **data engineering fundamentals**

---

## ğŸ“‚ Learning Index

### ğŸ”¹ Phase 1 â€“ PySpark & DataFrame Fundamentals

#### ğŸ“˜ `1_pyspark_rdd_select_withColumn_filter_distinct_dedup_to_date_date.pdf`
**Covers:**
- What is Apache Spark
- Spark vs Hadoop / MapReduce
- Spark architecture & execution flow
- RDD fundamentals and use cases
- RDD vs DataFrame comparison
- DataFrame basics
- Reading CSV & JSON files
- `select()` (column-based & index-based)
- `withColumn()` and `withColumnRenamed()`
- Data cleaning and type casting
- `filter()` conditions
- `distinct()` and `dropDuplicates()`
- Date handling using `to_date()` and `date_format()`
- Understanding data type vs display format

---

#### ğŸ§ª `1_select-withColumn-data_cleaning-data_handling-filter-groupBy-orderBy-limit.py`
**Practices:**
- Reading CSV data with and without schema inference
- Exploring DataFrame schema and columns
- Column selection (name-based & index-based)
- Cleaning numeric columns using `withColumn` and `regexp_replace`
- Casting columns to appropriate data types
- Date conversion and extracting year/month
- Creating calculated columns
- Filtering data using multiple conditions
- Aggregations using `groupBy` and `agg`
- Sorting and limiting results
- Answering basic business questions using PySpark
- Working with a real-world dataset (Adidas US Sales)

---

## ğŸ“ˆ Future Learning Phases
Additional learning phases will be added in the same structured format:
-
-
-
-
-
-
-




Each new phase will extend this README with:
- File name
- Covered concepts
- Practiced operations

---

## ğŸ› ï¸ Tools & Platform
- PySpark
- Databricks
- Apache Spark
- Real-world CSV datasets

---

## ğŸš§ Status
ğŸ”„ **Ongoing Learning Journey**  
This repository is actively updated as I progress through more advanced PySpark and Databricks topics.

---

## ğŸ“ Note
- Detailed explanations are intentionally kept inside the **PDF notes**
- Code files focus purely on **practice and implementation**
- This repository reflects **learning progression**, not production pipelines


